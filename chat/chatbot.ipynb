{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbOWKAb2aB2q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiPb6S9zZenn"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Cargar datos desde el archivo JSON\n",
        "with open('datos.json') as file:\n",
        "    datos = json.load(file)\n",
        "\n",
        "# Preprocesar los datos\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "palabras = []\n",
        "tags = []\n",
        "intents = datos['intents']\n",
        "\n",
        "for intent in intents:\n",
        "    for pattern in intent['patterns']:\n",
        "        # Tokenizar y lematizar las palabras\n",
        "        words = nltk.word_tokenize(pattern)\n",
        "        palabras.extend(words)\n",
        "        tags.append(intent['tag'])\n",
        "\n",
        "palabras = [lemmatizer.lemmatize(word.lower()) for word in palabras if word.isalnum()]\n",
        "palabras = sorted(list(set(palabras)))\n",
        "\n",
        "tags = sorted(list(set(tags)))\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "modelo = load_model('chatbot_modelo.h5')\n",
        "\n",
        "# Función para procesar el mensaje del usuario y obtener una respuesta del chatbot\n",
        "def procesar_mensaje(mensaje):\n",
        "    # Bolsa de palabras para el mensaje del usuario\n",
        "    bolsa_palabras = [lemmatizer.lemmatize(word.lower()) for word in nltk.word_tokenize(mensaje) if word.isalnum()]\n",
        "\n",
        "    # Codificación del mensaje en una matriz de entrada\n",
        "    entrada_usuario = [1 if palabra in bolsa_palabras else 0 for palabra in palabras]\n",
        "\n",
        "    # Predicción del modelo\n",
        "    resultado = modelo.predict(np.array([entrada_usuario]))\n",
        "\n",
        "    # Obtener la etiqueta predicha\n",
        "    predicha_tag = tags[np.argmax(resultado)]\n",
        "\n",
        "    # Buscar la respuesta asociada a la etiqueta\n",
        "    for intent in intents:\n",
        "        if intent['tag'] == predicha_tag:\n",
        "            respuesta = random.choice(intent['responses'])\n",
        "            return respuesta\n",
        "\n",
        "# Bucle de conversación\n",
        "print(\"¡Hola! Soy un chatbot. Puedes escribir 'salir' para terminar la conversación.\")\n",
        "\n",
        "while True:\n",
        "    # Obtener mensaje del usuario\n",
        "    mensaje_usuario = input(\"Usuario: \")\n",
        "\n",
        "    # Salir si el usuario escribe \"salir\"\n",
        "    if mensaje_usuario.lower() == 'salir':\n",
        "        print(\"¡Hasta luego!\")\n",
        "        break\n",
        "\n",
        "    # Procesar el mensaje y obtener la respuesta del chatbot\n",
        "    respuesta_chatbot = procesar_mensaje(mensaje_usuario)\n",
        "    print(\"Chatbot:\", respuesta_chatbot)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Cargar datos desde el archivo JSON\n",
        "with open('datos.json') as file:\n",
        "    datos = json.load(file)\n",
        "\n",
        "# Preprocesar los datos\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "palabras = []\n",
        "tags = []\n",
        "intents = datos['intents']\n",
        "\n",
        "for intent in intents:\n",
        "    for pattern in intent['patterns']:\n",
        "        # Tokenizar y lematizar las palabras\n",
        "        words = nltk.word_tokenize(pattern)\n",
        "        palabras.extend(words)\n",
        "        tags.append(intent['tag'])\n",
        "\n",
        "palabras = [lemmatizer.lemmatize(word.lower()) for word in palabras if word.isalnum()]\n",
        "palabras = sorted(list(set(palabras)))\n",
        "\n",
        "tags = sorted(list(set(tags)))\n",
        "\n",
        "# Crear conjuntos de datos de entrada y salida\n",
        "entrenamiento = []\n",
        "salida_vacia = [0] * len(tags)\n",
        "\n",
        "for intent in intents:\n",
        "    for pattern in intent['patterns']:\n",
        "        # Bolsa de palabras para cada patrón\n",
        "        bolsa_palabras = [lemmatizer.lemmatize(word.lower()) for word in nltk.word_tokenize(pattern) if word.isalnum()]\n",
        "\n",
        "        # Codificación de un patrón en una matriz de entrada\n",
        "        fila_entrenamiento = [1 if palabra in bolsa_palabras else 0 for palabra in palabras]\n",
        "\n",
        "        # Codificación de la etiqueta en una matriz de salida\n",
        "        etiqueta = list(salida_vacia)\n",
        "        etiqueta[tags.index(intent['tag'])] = 1\n",
        "\n",
        "        entrenamiento.append([fila_entrenamiento, etiqueta])\n",
        "\n",
        "random.shuffle(entrenamiento)\n",
        "entrenamiento = np.array(entrenamiento)\n",
        "\n",
        "# Separar datos de entrada y salida\n",
        "X_train = list(entrenamiento[:, 0])\n",
        "y_train = list(entrenamiento[:, 1])\n",
        "\n",
        "# Construir el modelo de red neuronal\n",
        "modelo = Sequential()\n",
        "modelo.add(Dense(128, input_shape=(len(X_train[0]),), activation='relu'))\n",
        "modelo.add(Dropout(0.5))\n",
        "modelo.add(Dense(64, activation='relu'))\n",
        "modelo.add(Dropout(0.5))\n",
        "modelo.add(Dense(len(y_train[0]), activation='softmax'))\n",
        "\n",
        "# Compilar el modelo\n",
        "modelo.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "for _ in range(5):\n",
        "    ##modelo.fit(np.array(X_train), np.array(y_train), epochs=1, batch_size=5, verbose=1)\n",
        "\n",
        "# Entrenar el modelo\n",
        "    historial_entrenamiento = modelo.fit(np.array(X_train), np.array(y_train), epochs=100, batch_size=5, verbose=1)\n",
        "\n",
        "# Guardar el modelo entrenado\n",
        "modelo.save('chatbot_modelo.h5')\n",
        "print(\"Modelo entrenado y guardado exitosamente.\")\n",
        "\n",
        "\n",
        "# Función para procesar el mensaje del usuario y obtener una respuesta del chatbot\n",
        "def procesar_mensaje(mensaje):\n",
        "    # Bolsa de palabras para el mensaje del usuario\n",
        "    bolsa_palabras = [lemmatizer.lemmatize(word.lower()) for word in nltk.word_tokenize(mensaje) if word.isalnum()]\n",
        "\n",
        "    # Codificación del mensaje en una matriz de entrada\n",
        "    entrada_usuario = [1 if palabra in bolsa_palabras else 0 for palabra in palabras]\n",
        "\n",
        "    # Predicción del modelo\n",
        "    resultado = modelo.predict(np.array([entrada_usuario]))\n",
        "\n",
        "    # Obtener la etiqueta predicha\n",
        "    predicha_tag = tags[np.argmax(resultado)]\n",
        "\n",
        "    # Buscar la respuesta asociada a la etiqueta\n",
        "    for intent in intents:\n",
        "        if intent['tag'] == predicha_tag:\n",
        "            respuesta = random.choice(intent['responses'])\n",
        "            return respuesta\n",
        "\n",
        "# Bucle de conversación\n",
        "print(\"¡Hola! Soy un chatbot. Puedes escribir 'salir' para terminar la conversación.\")\n",
        "\n",
        "while True:\n",
        "    # Obtener mensaje del usuario\n",
        "    mensaje_usuario = input(\"Usuario: \")\n",
        "\n",
        "    # Salir si el usuario escribe \"salir\"\n",
        "    if mensaje_usuario.lower() == 'salir':\n",
        "        print(\"¡Hasta luego!\")\n",
        "        break\n",
        "\n",
        "    # Procesar el mensaje y obtener la respuesta del chatbot\n",
        "    respuesta_chatbot = procesar_mensaje(mensaje_usuario)\n",
        "    print(\"Chatbot:\", respuesta_chatbot)\n"
      ],
      "metadata": {
        "id": "lDuicmB9lk3W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}